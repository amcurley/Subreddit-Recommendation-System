{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook Contents**\n",
    "\n",
    "- [Import Libraries](#importlibraries4)  \n",
    "- [Data Gathering](#gatherdata4)\n",
    "- [Future Steps](#steps4)  \n",
    "- [Citations](#cites4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"importlibraries4\"></a>\n",
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"gatherdata4\"></a>\n",
    "## **Data Gathering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Submission \n",
    "def submission_request_ml(**kwargs):\n",
    "    \n",
    "    # This does a get request to pull Submissions from the Machine Learning subreddit       #kwargs will be the params\n",
    "    request = requests.get(\"https://api.pushshift.io/reddit/search/submission/?subreddit=MachineLearning\", kwargs)\n",
    "    # This gets us data from the pull request\n",
    "    data = request.json()\n",
    "    # This gets us the individual 'DATA' for each post --> add [0] to get the first post\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_request_ai(**kwargs):\n",
    "    \n",
    "    # This does a get request to pull Submissions from the Machine Learning subreddit       #kwargs will be the params\n",
    "    request = requests.get(\"https://api.pushshift.io/reddit/search/submission/?subreddit=artificial\", kwargs)\n",
    "    # This gets us data from the pull request\n",
    "    data = request.json()\n",
    "    # This gets us the individual 'DATA' for each post --> add [0] to get the first post\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_ml = []\n",
    "title_ml = []\n",
    "selftext_ml =[]\n",
    "\n",
    "subreddit_ai = []\n",
    "title_ai = []\n",
    "selftext_ai =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dataframe for ml\n",
    "# pd.DataFrame(columns=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_ml = None\n",
    "before_ai = None\n",
    "iterations_times_25 = 2500  # This is the number of times it runs --> that number times 25 is how many rows we \"should\" get\n",
    "\n",
    "# Looping through the iterations \n",
    "while iterations_times_25 != 0:\n",
    "    time.sleep(1.5)\n",
    "    ml_submissions = submission_request_ml(before = before_ml, sort='desc', sort_type='created_utc')\n",
    "    ai_submissions = submission_request_ai(before = before_ai, sort='desc', sort_type='created_utc')\n",
    "\n",
    "#   MACHINE LEARNING\n",
    "    for xi in ml_submissions:\n",
    "        try:\n",
    "            before_ml = xi['created_utc']\n",
    "            \n",
    "            selftext_ml.append(xi['selftext'])\n",
    "            subreddit_ml.append(xi['subreddit'])\n",
    "            title_ml.append(xi['title'])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "#   ARTIFICAL INTELLIGENCE    \n",
    "    for n in ai_submissions:\n",
    "        try:\n",
    "            \n",
    "            before_ai = n['created_utc']\n",
    "            \n",
    "            selftext_ai.append(n['selftext'])\n",
    "            subreddit_ai.append(n['subreddit'])\n",
    "            title_ai.append(n['title'])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    time.sleep(1.5)\n",
    "    iterations_times_25 -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61648\n",
      "61648\n",
      "61648\n"
     ]
    }
   ],
   "source": [
    "print(len(subreddit_ml))\n",
    "print(len(title_ml))\n",
    "print(len(selftext_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31299\n",
      "31299\n",
      "31299\n"
     ]
    }
   ],
   "source": [
    "print(len(subreddit_ai))\n",
    "print(len(title_ai))\n",
    "print(len(selftext_ai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml = pd.DataFrame({'subreddit': subreddit_ml,\n",
    "                        'title': title_ml,\n",
    "                        'selftext': selftext_ml\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61648, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ai = pd.DataFrame({'subreddit': subreddit_ai,\n",
    "                        'title': title_ai,\n",
    "                        'selftext': selftext_ai\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31299, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ai.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will change based on how many rows we are able to get from reddit.\n",
    "data_ml = data_ml[:31299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31299, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31299, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ai.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.to_csv('./data/data_ml.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ai.to_csv('./data/data_ai.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"steps4\"></a>\n",
    "## **Future Steps**\n",
    "\n",
    "In the future I plan to make these functions able to scrape all of the 138,000 active subreddits and pull 2,000 posts from each subreddit. After this step I will have 276 million posts scraped and able to perform analysis on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cites4\"></a>\n",
    "## **Citations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit to David Lee for assitance with these steps  \n",
    "\n",
    "Crawling the submissions:  \n",
    "- https://www.textjuicer.com/2019/07/crawling-all-submissions-from-a-subreddit/\n",
    "\n",
    "Function for getting the submission:  \n",
    "- https://pythonprogramming.altervista.org/collect-data-from-reddit/?doing_wp_cron=1597670992.0452320575714111328125\n",
    "\n",
    "Countdown While loop for each iteration:  \n",
    "- https://datatofish.com/while-loop-python/\n",
    "\n",
    "Try and Except for 'selftext' problem:  \n",
    "- https://stackoverflow.com/questions/38707513/ignoring-an-error-message-to-continue-with-the-loop-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
